{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FPSULSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMYmmTMbUp2ctpW7ZacUiQv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Slaxzer96/klamkode/blob/master/FPSULSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKlZvXhWqMFm"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75xVxYg1tl4t"
      },
      "source": [
        "class AutoUpscale(nn.Module):\n",
        "    def __init__(self, input_size=6):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.BatchNorm2d(input_size),\n",
        "            nn.Conv2d(input_size,32,5,1,2),    # 32\n",
        "            nn.ReLU(),        \n",
        "            nn.BatchNorm2d(32),\n",
        "        \n",
        "            nn.Conv2d(32, 64, 3),              # 30\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "        \n",
        "            nn.Conv2d(32, 64, 3),              # 28\n",
        "            nn.ReLU())\n",
        "            \n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, 3),     # 30\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(32, 16, 3),     # 32\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(16, 8, 3, 1, 1),     # 32\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(8, 3, input_size//2, 1, 1),     # 32\n",
        "            nn.ReLU())\n",
        "\n",
        "    def decode(self, x: torch.Tensor):\n",
        "        return self.decode(x)\n",
        "\n",
        "    def encode(self, x: torch.Tensor):\n",
        "        return self.encode(x)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.decode(self.encode(x))\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"AutoUpscale_siamese\"\n",
        "        \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q6WD_uJvYpn"
      },
      "source": [
        "class FPSULSTM(nn.Module):\n",
        "    def __init__(self, d=512, n_lstm_layers=1, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(128, d)  # max(emails_ascii) < 128\n",
        "        self.lstm = nn.LSTM(d, d, n_lstm_layers, batch_first=True, dropout=dropout)\n",
        "        self.lin = nn.Linear(d, 1)\n",
        "\n",
        "    def forward(self, x, end_idx):  # (B, nx)\n",
        "        x = self.emb(x)  # (B, nx, d)\n",
        "        y = self.lstm(x)[0][torch.arange(len(x)), end_idx]  # (B, d)\n",
        "        return self.lin(y).view(-1)  # (B,)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKmapQA_3L9F"
      },
      "source": [
        "class ConvLSTM(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        self.ii = nn.Conv2d(input_size, input_size, 5, 1, 2)\n",
        "        self.ih = nn.Conv2d(input_size, input_size, 5, 1, 2)\n",
        "        self.ig = nn."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtbktIEfyzhE"
      },
      "source": [
        "all_losses = {}\n",
        "best_model = None\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8dymBCOylUz"
      },
      "source": [
        "\n",
        "autoUpscale = AutoUpscale(6).cuda()\n",
        "\n",
        "models = [autoUpscale]\n",
        "\n",
        "for model in models:\n",
        "    model.cuda()\n",
        "\n",
        "     opt = torch.optim.Adam(model.parameters(), lr=alpha)\n",
        "        train_losses = []\n",
        "        best_loss = float(\"inf\")\n",
        "\n",
        "        for epoch in tqdm(range(15)):\n",
        "            # train\n",
        "            epoch_losses = []\n",
        "\n",
        "            model.train()\n",
        "            for x, y in loader_train:\n",
        "                x = x.cuda()\n",
        "                reconstruction = model(x)\n",
        "                loss = F.l1_loss(reconstruction, y)\n",
        "\n",
        "                opt.zero_grad()\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "                mae = F.l1_loss(reconstruction, x)\n",
        "\n",
        "                epoch_losses.append(loss.item())\n",
        "\n",
        "            train_loss = np.mean(epoch_losses)\n",
        "            \n",
        "            print(f' rmse: {np.sqrt(loss.item()):.4f} - mae: {mae.item():.4f}')\n",
        "\n",
        "            #if train_loss < best_loss:\n",
        "            #    best_loss = train_loss\n",
        "            #best_model = copy.deepcopy(model), epoch\n",
        "\n",
        "            train_losses.append(train_loss)\n",
        "\n",
        "\n",
        "        all_losses[alpha, model] = train_losses\n",
        "\n",
        "model.eval()\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "for n, (train_losses) in all_losses.items():\n",
        "  p = plt.plot(train_losses, label=f'{n}:train')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}